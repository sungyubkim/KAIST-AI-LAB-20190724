{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 실습에서는 Bayesian Neural Network(BNN)와 Variational Inference (VI)의 개념을 복습하고, 이를 deep learning에서 구현하는 방법인 Monte-Carlo DropOut (MCDO)의 구현을 알아봅니다. 또한, MCDO를 Dense layer가 아닌 CNN, RNN 등의 구조에서 적용하는 방법에 대해서도 알아봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap : Bayesian NN (BNN) and Variational Inference (VI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 deep learning의 학습은 다음과 같이 정의됩니다.\n",
    "\n",
    "* 네트워크 구조 정의 및 파라미터 초기화\n",
    "* Loss function 정의\n",
    "* Loss function을 최소화 시키는 파라미터 찾기(with SGD)\n",
    "\n",
    "이러한 학습 과정은 Loss function을 최소화 시키는 하나의 파라미터 값을 찾기 때문에 빈도주의적 접근(Frequentist approach)라고 표현합니다. 그렇다면 이 과정을 어떻게 사후 분포를 찾는 Bayesian approach로 변화시킬 수 있을까요? 이를 위해서는 먼저 파라미터의 분포 ($p(\\theta)$)를 정의하고 위 과정을 다음과 같이 수정하는 방식으로 달성할 수 있습니다.\n",
    "\n",
    "\n",
    "* 네트워크 구조 정의 및 파라미터의 사전 분포 ($p(\\theta)$) 정의\n",
    "* Likelihood function ($p(\\mathcal{D}|\\theta)$)정의\n",
    "* Bayes Theorem($p(\\theta| \\mathcal{D}) = \\frac{p(\\theta,\\mathcal{D})}{p(\\mathcal{D})}$)을 적용하여 파라미터의 사후분포 찾기\n",
    "\n",
    "이 과정에서 Bayes Theorem은 다음과 같은 적분을 필요로 하며 이를 계산하는 것은 매우 어렵습니다.\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}) = \\int_{\\Omega} p(\\mathcal{D} | \\theta) p(\\theta) d\\theta\n",
    "$$\n",
    "\n",
    "따라서 이 계산을 우회하기 위해 우리는 파라미터의 사후 분포 ($p(\\theta | \\mathcal{D}$)를 다음과 같이 해석할 수 있습니다.\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D} ) = \\arg \\inf_{q} \\Big\\{ -\\int_{\\Omega} p(\\mathcal{D} | \\theta ) q(\\theta)d\\theta + D_{KL}(q(\\theta)\\| p(\\theta)) \\Big\\}\n",
    "$$\n",
    "\n",
    "이때 $\\arg \\inf_{q} \\Big\\{-\\int_{\\Omega} p(\\mathcal{D} | \\theta ) q(\\theta)d\\theta \\Big\\}$는 제안된 분포 $q(\\theta)$가 얼마나 likelihood function $p(\\mathcal{D} | \\theta )$을 최대화 시키는지를 나타내며,\n",
    "$\\arg \\inf_{q} \\Big\\{D_{KL}(q(\\theta)\\| p(\\theta)) \\Big\\}$는 제안된 분포 $q(\\theta)$가 얼마나 prior $p(\\theta)$에 가까운지를 나타냅니다.\n",
    "\n",
    "즉, 우리가 나타내고자 하는 사후 분포 $p(\\theta | \\mathcal{D})$는 데이터에 대한 설명력을 높이면서 동시에 사전 분포에 가깝게 하는 문제를 **임의의 파라미터 분포**에 대해서 찾은 답을 의미합니다.\n",
    "\n",
    "한편 **임의의 파라미터 분포**는 우리가 최적화하기 어려운 대상이므로 이를 최적화하기 쉬운 좀더 작은 분포로 나타낼 수 있습니다. 예를 들어, 파라미터의 분포를 \n",
    "\n",
    "* 각 파라미터가 서로 독립적이고,\n",
    "* 각 파라미터가 Gaussian 분포를 따른다고\n",
    "\n",
    "가정하게 되면 해당 분포는 **각 Gaussian 분포의 평균과 표준편차**만으로 표현할 수 있게 됩니다. 그리고 이러한 분포들 중에서 위 목적 함수를 최대화하는 분포는 각 Gaussian의 평균과 표준편차를 최적화(with SGD)함으로써 찾을 수 있습니다. 이렇게 **임의의 파라미터 분포**에서 최적화하는 사후 분포와 달리, 변분 추론(variational inference, VI)는 우리가 **최적화하기 적합한 더 작은 분포들**에 대해서 최적화하여 근사적으로 사후 분포를 찾는 방법이라고 할 수 있습니다. 정리하면 BNN의 VI는 다음과 같이 적용할 수 있습니다.\n",
    "\n",
    "* 네트워크 구조 정의 및 파라미터의 사전 분포 ($p(\\theta)$) 정의\n",
    "* Likelihood function ($p(\\mathcal{D}|\\theta)$)정의\n",
    "* 최적화 하고자 하는 분포의 범위(e.g. 서로 독립적인 Gaussian) 정의\n",
    "* Negative likelihood + Prior의 KL divergence가 최소화 되는 파라미터 찾기(with SGD)\n",
    "\n",
    "앞으로 설명하게 될 MCDO는 최적화 하고자 하는 분포의 범위를 **DropOut으로 표현 가능한 모든 파라미터의 분포**라고 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap : DropOut and Monte-Carlo DropOut (MCDO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 DropOut은 다음과 같이 train/test time의 네트워크를 다르게 activate 시킵니다.\n",
    "\n",
    "* Train : 각 레이어의 input을 정해진 확률 $p$만큼 random으로 drop하여 0으로 만들고 계산.(\"thinned\" networks)\n",
    "* Test : 각 레이어의 모든 input을 계산하되, weight matrix에 $p$만큼 곱하여 evaluation.\n",
    "\n",
    "![](./images/dropout.png)\n",
    "\n",
    "한편 MCDO에서는 train/test time의 네트워클 모두 동일하게 dropout하여 사용합니다. 즉, MCDO는 test time에도 DropOut이 적용되는 input에 따라 다른 결과값이 계산되며 이로부터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCDO on FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network strucuture of this tutorial is based on \n",
    "# https://github.com/jwlee-ml/Tensorflow_FastCampus_9th\n",
    "\n",
    "# import tf and set random seed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, n_hidden=512, n_output=10):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # define placeholders for training MLP\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.dropout_rate = tf.placeholder(tf.float32)\n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.logit, self.loss, self.opt = self.build_graph(n_hidden=self.n_hidden, n_output=self.n_output)\n",
    "\n",
    "    def build_graph(self, n_hidden=512, n_output=10):\n",
    "        \"\"\"\n",
    "        Build computational graph for MNIST training with DropOut.\n",
    "\n",
    "        Args:\n",
    "            x : MINST input data. (m, 784)\n",
    "            n_hidden : the number of hidden units in MLP.\n",
    "            n_output : the size of output layer (=10)\n",
    "            rate: dropout rate for dropout layers in MLP (tf.placeholder)\n",
    "\n",
    "        Returns:\n",
    "            logit : log-probability of prediction. (m, 10)\n",
    "            loss : cross entropy loss \n",
    "            learning rate : learning rate of Adam\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope('mlp'):\n",
    "            # initializers for weight and bias\n",
    "            w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            b_init = tf.constant_initializer(0.)\n",
    "\n",
    "            # 1st hidden layer\n",
    "            w0 = tf.get_variable('w0', [self.x.get_shape()[1], n_hidden], initializer=w_init)\n",
    "            b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)\n",
    "            h0 = tf.matmul(self.x, w0) + b0\n",
    "            h0 = tf.nn.relu(h0)\n",
    "            h0 = tf.nn.dropout(h0, rate=self.dropout_rate)\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)\n",
    "            b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)\n",
    "            h1 = tf.matmul(h0, w1) + b1\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            h1 = tf.nn.dropout(h1, rate=self.dropout_rate)\n",
    "\n",
    "            # 3nd hidden layer\n",
    "            w2 = tf.get_variable('w2', [h1.get_shape()[1], n_hidden], initializer=w_init)\n",
    "            b2 = tf.get_variable('b2', [n_hidden], initializer=b_init)\n",
    "            h2 = tf.matmul(h1, w2) + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            h2 = tf.nn.dropout(h2, rate=self.dropout_rate)\n",
    "\n",
    "            # 4nd hidden layer\n",
    "            w3 = tf.get_variable('w3', [h2.get_shape()[1], n_hidden], initializer=w_init)\n",
    "            b3 = tf.get_variable('b3', [n_hidden], initializer=b_init)\n",
    "            h3 = tf.matmul(h2, w3) + b3\n",
    "            h3 = tf.nn.relu(h3)\n",
    "            h3 = tf.nn.dropout(h3, rate=self.dropout_rate)\n",
    "\n",
    "            # output layer\n",
    "            wo = tf.get_variable('wo', [h3.get_shape()[1], n_output], initializer=w_init)\n",
    "            bo = tf.get_variable('bo', [n_output], initializer=b_init)\n",
    "            logit = tf.matmul(h3, wo) + bo\n",
    "            \n",
    "            # loss\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logit, labels=self.y))\n",
    "            \n",
    "            # optimizer\n",
    "            opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss)\n",
    "\n",
    "        return logit, loss, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 11:40:08.360730 4614247872 deprecation.py:323] From <ipython-input-3-2639ea0d9fc8>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0716 11:40:08.361680 4614247872 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0716 11:40:08.362490 4614247872 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0716 11:40:08.534732 4614247872 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0716 11:40:08.536417 4614247872 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 11:40:08.577684 4614247872 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "sess = tf.Session()\n",
    "# get MINST data\n",
    "mnist = input_data.read_data_sets(\"./data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MNIST(net, sess, num_epoch=15, batch_size=100, data=mnist):\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # iterating epoch\n",
    "    for epoch in range(num_epoch):\n",
    "        avg_loss = 0.\n",
    "        num_batch = int(data.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(num_batch):\n",
    "            # get minibatch of X,Y\n",
    "            batch_xs, batch_ys = data.train.next_batch(batch_size)\n",
    "            feed_dict = {net.x: batch_xs,\n",
    "                         net.y: batch_ys,\n",
    "                         net.dropout_rate: 0.3, \n",
    "                         net.lr : 1e-3}\n",
    "            l, _ = sess.run([net.loss, net.opt], feed_dict=feed_dict)\n",
    "            avg_loss += l / num_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_loss))\n",
    "\n",
    "    print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def evaluate_MNIST(net, sess, data=mnist):\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(net.logit, 1), tf.argmax(net.y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('DropOut Accuracy:', sess.run(accuracy, feed_dict={net.x: data.test.images,\n",
    "                                                             net.y: data.test.labels,\n",
    "                                                             net.dropout_rate: 0.0}))\n",
    "    \n",
    "    logits = list()\n",
    "    for i in range(30):\n",
    "        logits.append(sess.run(net.logit, feed_dict={net.x: data.test.images,\n",
    "                                                     net.y: data.test.labels,\n",
    "                                                     net.dropout_rate: 0.3}))\n",
    "    logit = np.mean(np.array(logits), 0)\n",
    "    correct_prediction = np.equal(np.argmax(logit, 1), np.argmax(data.test.labels, 1))\n",
    "    accuracy = np.mean(correct_prediction)\n",
    "    print('MCDO Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.365000198\n",
      "Epoch: 0002 cost = 0.162444700\n",
      "Epoch: 0003 cost = 0.126597639\n",
      "Epoch: 0004 cost = 0.105118152\n",
      "Epoch: 0005 cost = 0.092673498\n",
      "Epoch: 0006 cost = 0.080030959\n",
      "Epoch: 0007 cost = 0.076510305\n",
      "Epoch: 0008 cost = 0.070558808\n",
      "Epoch: 0009 cost = 0.062834471\n",
      "Epoch: 0010 cost = 0.058524062\n",
      "Epoch: 0011 cost = 0.054050435\n",
      "Epoch: 0012 cost = 0.053791622\n",
      "Epoch: 0013 cost = 0.052053770\n",
      "Epoch: 0014 cost = 0.047733511\n",
      "Epoch: 0015 cost = 0.045533521\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "train_MNIST(mlp, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropOut Accuracy: 0.9822\n",
      "MCDO Accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "evaluate_MNIST(mlp, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCDO on CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self, kernel_size=3, n_hidden=512, n_output=10):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # define placeholders for training MLP\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.x_img = tf.reshape(self.x, [-1, 28, 28, 1])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.dropout_rate = tf.placeholder(tf.float32)\n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.logit, self.loss, self.opt = self.build_graph(kernel_size=self.kernel_size, n_hidden=self.n_hidden, n_output=self.n_output)\n",
    "\n",
    "    def build_graph(self, kernel_size=3, n_hidden=512, n_output=10):\n",
    "        \"\"\"\n",
    "        Build computational graph for MNIST training with DropOut.\n",
    "\n",
    "        Args:\n",
    "            x : MINST input data. (m, 784)\n",
    "            n_hidden : the number of hidden units in MLP.\n",
    "            n_output : the size of output layer (=10)\n",
    "            rate: dropout rate for dropout layers in MLP (tf.placeholder)\n",
    "\n",
    "        Returns:\n",
    "            logit : log-probability of prediction. (m, 10)\n",
    "            loss : cross entropy loss \n",
    "            learning rate : learning rate of Adam\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope('cnn'):\n",
    "            strides = [1, 1, 1, 1]\n",
    "            # initializers for weight and bias\n",
    "            w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            b_init = tf.constant_initializer(0.)\n",
    "\n",
    "            # 1st hidden layer : (m, 28, 28, 1) -> (m, 14, 14, 32)\n",
    "            w0 = tf.get_variable('w0', [kernel_size, kernel_size, 1, 32], initializer=w_init)\n",
    "            h0 = tf.nn.conv2d(self.x_img, w0, strides=strides, padding='SAME')\n",
    "            h0 = tf.nn.relu(h0)\n",
    "            h0 = tf.nn.max_pool(h0, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            h0 = tf.nn.dropout(h0, rate=self.dropout_rate)\n",
    "\n",
    "            # 2nd hidden layer : (m, 14, 14, 32) -> (m, 7, 7, 64)\n",
    "            w1 = tf.get_variable('w1', [kernel_size, kernel_size, 32, 64], initializer=w_init)\n",
    "            h1 = tf.nn.conv2d(h0, w1, strides=strides, padding='SAME')\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            h1 = tf.nn.max_pool(h1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            h1 = tf.nn.dropout(h1, rate=self.dropout_rate)\n",
    "\n",
    "            # 3nd hidden layer : (m, 7, 7, 64) -> (m, 4, 4, 128)\n",
    "            w2 = tf.get_variable('w2', [kernel_size, kernel_size, 64, 128], initializer=w_init)\n",
    "            h2 = tf.nn.conv2d(h1, w2, strides=strides, padding='SAME')\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            h2 = tf.nn.max_pool(h2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            h2 = tf.nn.dropout(h2, rate=self.dropout_rate)\n",
    "            \n",
    "            # reshape : (m, 4, 4, 128) -> (m, 4 * 4 * 128)\n",
    "            h2 = tf.reshape(h2, [-1, 4 * 4 * 128])\n",
    "\n",
    "            # 4nd hidden layer : (m, 4 * 4 * 128) -> (m, n_hidden)\n",
    "            w3 = tf.get_variable('w3', [h2.get_shape()[1], n_hidden], initializer=w_init)\n",
    "            b3 = tf.get_variable('b3', [n_hidden], initializer=b_init)\n",
    "            h3 = tf.matmul(h2, w3) + b3\n",
    "            h3 = tf.nn.relu(h3)\n",
    "            h3 = tf.nn.dropout(h3, rate=self.dropout_rate)\n",
    "\n",
    "            # output layer : (m, n_hidden) -> (m, n_output)\n",
    "            wo = tf.get_variable('wo', [h3.get_shape()[1], n_output], initializer=w_init)\n",
    "            bo = tf.get_variable('bo', [n_output], initializer=b_init)\n",
    "            logit = tf.matmul(h3, wo) + bo\n",
    "            \n",
    "            # loss\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logit, labels=self.y))\n",
    "            \n",
    "            # optimizer\n",
    "            opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss)\n",
    "\n",
    "        return logit, loss, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.376906734\n",
      "Epoch: 0002 cost = 0.109382831\n",
      "Epoch: 0003 cost = 0.076635950\n",
      "Epoch: 0004 cost = 0.064737637\n",
      "Epoch: 0005 cost = 0.054634125\n",
      "Epoch: 0006 cost = 0.049057720\n",
      "Epoch: 0007 cost = 0.044841536\n",
      "Epoch: 0008 cost = 0.041880070\n",
      "Epoch: 0009 cost = 0.041395386\n",
      "Epoch: 0010 cost = 0.036367649\n",
      "Epoch: 0011 cost = 0.034328564\n",
      "Epoch: 0012 cost = 0.033231129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0716 11:47:52.877957 4614247872 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-26433b7db5bc>\", line 1, in <module>\n",
      "    train_MNIST(cnn, sess)\n",
      "  File \"<ipython-input-4-3381bfae259a>\", line 15, in train_MNIST\n",
      "    l, _ = sess.run([net.loss, net.opt], feed_dict=feed_dict)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/anaconda3/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/anaconda3/lib/python3.7/posixpath.py\", line 87, in join\n",
      "    if b.startswith(sep):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "train_MNIST(cnn, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropOut Accuracy: 0.9939\n",
      "MCDO Accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "evaluate_MNIST(cnn, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCDO on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, kernel_size=3, n_hidden=512, n_output=10):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # define placeholders for training MLP\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.x_img = tf.reshape(self.x, [-1, 28, 28])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.dropout_rate = tf.placeholder(tf.float32)\n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.logit, self.loss, self.opt = self.build_graph(kernel_size=self.kernel_size, n_hidden=self.n_hidden, n_output=self.n_output)\n",
    "\n",
    "    def build_graph(self, kernel_size=3, n_hidden=512, n_output=10):\n",
    "        \"\"\"\n",
    "        Build computational graph for MNIST training with DropOut.\n",
    "\n",
    "        Args:\n",
    "            x : MINST input data. (m, 784)\n",
    "            n_hidden : the number of hidden units in MLP.\n",
    "            n_output : the size of output layer (=10)\n",
    "            rate: dropout rate for dropout layers in MLP (tf.placeholder)\n",
    "\n",
    "        Returns:\n",
    "            logit : log-probability of prediction. (m, 10)\n",
    "            loss : cross entropy loss \n",
    "            learning rate : learning rate of Adam\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            # initializers for weight and bias\n",
    "            w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            b_init = tf.constant_initializer(0.)\n",
    "\n",
    "            # rnn cell\n",
    "            rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_hidden)\n",
    "            keep_prob = 1. - self.dropout_rate\n",
    "            var_rnn_cell = tf.contrib.rnn.DropoutWrapper(rnn_cell,\n",
    "                                                        input_keep_prob=keep_prob,\n",
    "                                                        output_keep_prob=keep_prob,\n",
    "                                                        state_keep_prob=keep_prob,)\n",
    "            outputs, _ = tf.nn.dynamic_rnn(rnn_cell, self.x_img, dtype=tf.float32)\n",
    "\n",
    "            # output layer\n",
    "            wo = tf.get_variable('wo', [n_hidden, n_output], initializer=w_init)\n",
    "            bo = tf.get_variable('bo', [n_output], initializer=b_init)\n",
    "            last_rnn_output = outputs[:, -1, :]\n",
    "            logit = tf.matmul(last_rnn_output, wo) + bo\n",
    "            \n",
    "            # loss\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logit, labels=self.y))\n",
    "            \n",
    "            # optimizer\n",
    "            opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss)\n",
    "\n",
    "        return logit, loss, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 12:07:45.806266 4436415936 deprecation.py:323] From <ipython-input-2-ce9e6a25182e>:39: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0716 12:07:45.809188 4436415936 deprecation.py:323] From <ipython-input-2-ce9e6a25182e>:45: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0716 12:07:45.920722 4436415936 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0716 12:07:45.930181 4436415936 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MNIST(rnn, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_MNIST(rnn, sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
